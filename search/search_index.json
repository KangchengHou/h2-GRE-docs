{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"h2-GRE \u00b6 h2-GRE is a software package implementing method for accurate estimation of SNP-heritability from biobank-scale data. Estimating h2-GRE consists of the following steps, please click link for a tutorial for each part. In these tutorial, we are aiming to introduce each step. So for simplicity, the scripts are in sequential version, i.e., these scripts do not utilize the parallel computing on the cluster. Prepare the data . Perform GWAS (i.e. OLS regression) using individual level data. Compute chromosome-wide pseudoinverse of LD . Calculate h2-GRE . We also prepare a step-by-step tutorial which demonstrate how we performed analyses on UK Biobank data (#individuals=337k, genome-wide #SNPs=600k, largest chromosome #SNPs=60k). Details on each step can be also found in our paper. Software requirement \u00b6 python 3 python-fire pysnptools numpy pandas scipy References \u00b6 Kangcheng Hou*, Kathryn S. Burch*, Arunabha Majumdar, Huwenbo Shi, Nicholas Mancuso, Yue Wu, Sriram Sankararaman & Bogdan Pasaniuc. \"Accurate estimation of SNP-heritability from biobank-scale data irrespective of genetic architecture\" [PDF] Contact \u00b6 Kangcheng Hou: kangchenghou at gmail dot com","title":"Home"},{"location":"#h2-gre","text":"h2-GRE is a software package implementing method for accurate estimation of SNP-heritability from biobank-scale data. Estimating h2-GRE consists of the following steps, please click link for a tutorial for each part. In these tutorial, we are aiming to introduce each step. So for simplicity, the scripts are in sequential version, i.e., these scripts do not utilize the parallel computing on the cluster. Prepare the data . Perform GWAS (i.e. OLS regression) using individual level data. Compute chromosome-wide pseudoinverse of LD . Calculate h2-GRE . We also prepare a step-by-step tutorial which demonstrate how we performed analyses on UK Biobank data (#individuals=337k, genome-wide #SNPs=600k, largest chromosome #SNPs=60k). Details on each step can be also found in our paper.","title":"h2-GRE"},{"location":"#software-requirement","text":"python 3 python-fire pysnptools numpy pandas scipy","title":"Software requirement"},{"location":"#references","text":"Kangcheng Hou*, Kathryn S. Burch*, Arunabha Majumdar, Huwenbo Shi, Nicholas Mancuso, Yue Wu, Sriram Sankararaman & Bogdan Pasaniuc. \"Accurate estimation of SNP-heritability from biobank-scale data irrespective of genetic architecture\" [PDF]","title":"References"},{"location":"#contact","text":"Kangcheng Hou: kangchenghou at gmail dot com","title":"Contact"},{"location":"biobank_demo/","text":"SNP-heritability estimation for Biobank \u00b6 Now we demonstrate a walkthrough on how to estimate SNP-heritability for GWAS in Biobank scale (e.g. UK Biobank) starting from genotype and phenotype data. Data \u00b6 Prepare the following data: all_bfile : Genotype for the unrelated individuals in UK Biobank after quality control. This contains genome-wide data in one file. pheno_file : the phenotype file. covar_file : the covariates which will be used for association studies. Data requirement for GRE GRE is designed for estimating SNP-heritability at Biobank scale. Only genotyped SNPs should be included. Also make sure that number of individuals > number of snps on each chromosome . In our simulation studies , we find when the #individuals=337k, genome-wide #SNPs=600k, largest chromosome #SNPs=60k, GRE works great. Variables in the script \u00b6 Here is a summary of variables we will use in the following script and also the file structure we are going to create. Note on the scripts Note that there are some parameters about the memory and time shown in the scripts below. These parameters are ones we used to perform analyses with data #individuals=337k, #genome-wide SNPs=600k. You might need to tune them according to the size of the data. But be sure to note the consistency. If you have questions, feel free to contact us. Also, since h2-GRE is designed to analyze biobank-scale data, we assume you have a computing cluster in your organization. And note that the commands for submitting jobs (we use qsub command) to the computing cluster may vary between different sysmtems. Association summary statistics \u00b6 To use GRE, be sure to perform the association studies with the exactly same genotype you use to compute the LD. We leave GRE with reference panel LD for future work. We compute the OLS sumstats using plink. First create a bash script called assoc.sh . #!/bin/sh #$ -l h_data=20G,h_rt=24:00:00,highp #$ -cwd #$ -j y #$ -o ./job_out #$ -t 1-22:1 source /u/local/Modules/default/init/modules.sh module load plink chr_i=$SGE_TASK_ID num_cols=$(head -1 $covar_file | awk '{print NF}') num_cols=$((num_cols-2)) mkdir -p ./assoc plink --bfile ${all_bfile} \\ --chr $chr_i \\ --linear hide-covar \\ --ci 0.95 \\ --pheno ${pheno_file} \\ --allow-no-sex \\ --covar ${covar_file} \\ --covar-number 1-${num_cols} \\ --out ./assoc/chr${chr_i} qsub assoc.sh You should get 22 association file after this step. Each contains the OLS sumstats for SNPs in 22 chromosome. Estimating using GRE \u00b6 Divide the genotype into 22 parts by chromosome using plink \u00b6 # now in root directory mkdir genotype for chr_i in $(seq 1 22) do plink \\ --bfile ${all_bfile} --chr ${chr_i} --make-bed --out genotype/chr${chr_i} done Download GRE \u00b6 git clone https://github.com/bogdanlab/h2-GRE.git Create 22 directory to store intermediate files used by GRE \u00b6 # now in root directory mkdir ld for chr_i in $(seq 1 22) do mkdir ld/chr${chr_i} done Compute the mean and standard deviation for each genotype file \u00b6 mean_std.sh #!/bin/sh #$ -cwd #$ -j y #$ -l h_data=8G,h_rt=1:00:00 #$ -o ./job_out #$ -t 1-22 chr_i=$SGE_TASK_ID bfile=./genotype/chr${chr_i} ld_dir=./ld/chr${chr_i} chunk_size=500 gre_path=./h2-GRE/gre/gre.py python ${gre_path} mean-std --bfile=${bfile} --chunk-size=${chunk_size} --ld-dir=${ld_dir} qsub mean_std.sh This step will produce a file containing the mean and standard deviation for each genotype file. In the following few steps, we compute the LD matrix efficiently. We achieve this by seperating the big task, e.g., computing the 60k x 60k SNP covariance matrix into computing 3600 1k x 1k SNP covariance matrix. Cutting the task of computing LD matrix into several tasks \u00b6 for chr_i in $(seq 1 22) do python ${gre_path} cut-ld --bfile ./genotype/chr${chr_i} --chunk-size 1000 --ld-dir ./ld/chr${chr_i} python ${gre_path} check-ld ./ld/chr${chr_i} done This will output two files part.info , part.missing in each directory ./ld/chr${chr_i} , which specifies the sub tasks needed to compute the LD. Computing each part of covariance matrix \u00b6 First create two scripts, calc_ld.sh , run_calc_ld.sh . calc_ld.sh #!/bin/sh #$ -cwd #$ -l h_data=16G,h_rt=0:40:00 #$ -j y #$ -o ./job_out chr_i=$1 batch_size=$2 bfile=./genotype/chr${chr_i} ld_dir=./ld/${chr_i} gre_path=./h2-GRE/gre/gre.py for i in $(seq 1 ${batch_size}) do line=$((SGE_TASK_ID + i - 1)) # part.missing contains undone tasks task_id=$(awk 'NR == n' n=$line ${ld_dir}/part.missing) python ${gre_path} calc-ld --bfile=${bfile} --part-i=${task_id} --ld-dir=${ld_dir} done run_calc_ld.sh batch_size=25 for chr_i in $(seq 1 22) do ld_dir=./ld/${chr_i} part_num=$(wc -l < ${ld_dir}/part.missing) if [ ${part_num} -ne 0 ]; then qsub -t 1-${part_num}:${batch_size} calc_ld.sh ${chr_i} ${batch_size} fi done done Then run the run_calc_ld.sh to submit the jobs. bash run_calc_ld.sh check if jobs are done and resubmit the jobs if necessary \u00b6 Some of jobs might not finish due to slow nodes on the cluster. We update the part.missing list first. for chr_i in $(seq 1 22) do python ${gre_path} check-ld --ld-dir ./ld/chr${chr_i} done And calculate the ld for each part by bash run_calc_ld.sh . Repeat this until you see no tasks being submitted. Merging parts of the covariance matrix \u00b6 Now we are close to finishing it! We will merge each parts of LD and compute the pseudo-inverse of LD. merge_ld.sh #!/bin/sh #$ -cwd #$ -j y #$ -o ./job_out #$ -m a bfile=$1 ld_dir=$2 gre_path=./h2-GRE/gre/gre.py python ${gre_path} merge-ld --bfile=${bfile} --ld-dir=${ld_dir} for chr_i in $(seq 1 22) do ld_dir=./ld/chr${chr_i} bfile=./genotype/chr${chr_i} if [ $chr_i -le 6 ] then qsub -l h_data=60G,h_rt=4:00:00,highp merge_ld.sh ${bfile} ${ld_dir} elif [ $chr_i -ge 7 -a $chr_i -le 12 ] then qsub -l h_data=40G,h_rt=3:00:00,highp merge_ld.sh ${bfile} ${ld_dir} elif [ $chr_i -ge 13 -a $chr_i -le 20 ] then qsub -l h_data=24G,h_rt=2:00:00,highp merge_ld.sh ${bfile} ${ld_dir} else qsub -l h_data=10G,h_rt=1:00:00,highp merge_ld.sh ${bfile} ${ld_dir} fi done Now we have all the ingredients needed for GRE and we are ready to get the SNP-heritability. Estimating SNP-heritability \u00b6 Converting the sumstats in plink format into LDSC format. for chr_i in $(seq 1 22) do python ./h2-GRE/utils/utils.py sumstats-plink2ldsc --plink-sumstats ./assoc/chr${chr_i}.assoc.linear --legend ./genotype/chr${chr_i}.bim --out ./assoc/chr${chr_i}.ldsc.txt done Estimate SNP-heritability estimate.sh #!/bin/sh #$ -cwd #$ -l h_data=24G,h_rt=2:00:00,highp #$ -j y #$ -o ./job_out #$ -t 1-22:1 chr_i=${SGE_TASK_ID} legend=./genotype/chr${chr_i}.bim ld_dir=./ld/chr${chr_i} sumstats=./assoc/chr${chr_i}.ldsc.txt python gre.py estimate --legend=${legend} --ld-dir=${ld_dir} --sumstats=${sumstats} > ./estimate/chr${chr_i}.txt qsub estimate.sh ./estimate/chr${chr_i}.txt contains the estimate and standard deviation of GRE for each chromosome. Adding these up provides you the genome-wide SNP-heritability!","title":"Demo"},{"location":"biobank_demo/#snp-heritability-estimation-for-biobank","text":"Now we demonstrate a walkthrough on how to estimate SNP-heritability for GWAS in Biobank scale (e.g. UK Biobank) starting from genotype and phenotype data.","title":"SNP-heritability estimation for Biobank"},{"location":"biobank_demo/#data","text":"Prepare the following data: all_bfile : Genotype for the unrelated individuals in UK Biobank after quality control. This contains genome-wide data in one file. pheno_file : the phenotype file. covar_file : the covariates which will be used for association studies. Data requirement for GRE GRE is designed for estimating SNP-heritability at Biobank scale. Only genotyped SNPs should be included. Also make sure that number of individuals > number of snps on each chromosome . In our simulation studies , we find when the #individuals=337k, genome-wide #SNPs=600k, largest chromosome #SNPs=60k, GRE works great.","title":"Data"},{"location":"biobank_demo/#variables-in-the-script","text":"Here is a summary of variables we will use in the following script and also the file structure we are going to create. Note on the scripts Note that there are some parameters about the memory and time shown in the scripts below. These parameters are ones we used to perform analyses with data #individuals=337k, #genome-wide SNPs=600k. You might need to tune them according to the size of the data. But be sure to note the consistency. If you have questions, feel free to contact us. Also, since h2-GRE is designed to analyze biobank-scale data, we assume you have a computing cluster in your organization. And note that the commands for submitting jobs (we use qsub command) to the computing cluster may vary between different sysmtems.","title":"Variables in the script"},{"location":"biobank_demo/#association-summary-statistics","text":"To use GRE, be sure to perform the association studies with the exactly same genotype you use to compute the LD. We leave GRE with reference panel LD for future work. We compute the OLS sumstats using plink. First create a bash script called assoc.sh . #!/bin/sh #$ -l h_data=20G,h_rt=24:00:00,highp #$ -cwd #$ -j y #$ -o ./job_out #$ -t 1-22:1 source /u/local/Modules/default/init/modules.sh module load plink chr_i=$SGE_TASK_ID num_cols=$(head -1 $covar_file | awk '{print NF}') num_cols=$((num_cols-2)) mkdir -p ./assoc plink --bfile ${all_bfile} \\ --chr $chr_i \\ --linear hide-covar \\ --ci 0.95 \\ --pheno ${pheno_file} \\ --allow-no-sex \\ --covar ${covar_file} \\ --covar-number 1-${num_cols} \\ --out ./assoc/chr${chr_i} qsub assoc.sh You should get 22 association file after this step. Each contains the OLS sumstats for SNPs in 22 chromosome.","title":"Association summary statistics"},{"location":"biobank_demo/#estimating-using-gre","text":"","title":"Estimating  using GRE"},{"location":"biobank_demo/#divide-the-genotype-into-22-parts-by-chromosome-using-plink","text":"# now in root directory mkdir genotype for chr_i in $(seq 1 22) do plink \\ --bfile ${all_bfile} --chr ${chr_i} --make-bed --out genotype/chr${chr_i} done","title":"Divide the genotype into 22 parts by chromosome using plink"},{"location":"biobank_demo/#download-gre","text":"git clone https://github.com/bogdanlab/h2-GRE.git","title":"Download GRE"},{"location":"biobank_demo/#create-22-directory-to-store-intermediate-files-used-by-gre","text":"# now in root directory mkdir ld for chr_i in $(seq 1 22) do mkdir ld/chr${chr_i} done","title":"Create 22 directory to store intermediate files used by GRE"},{"location":"biobank_demo/#compute-the-mean-and-standard-deviation-for-each-genotype-file","text":"mean_std.sh #!/bin/sh #$ -cwd #$ -j y #$ -l h_data=8G,h_rt=1:00:00 #$ -o ./job_out #$ -t 1-22 chr_i=$SGE_TASK_ID bfile=./genotype/chr${chr_i} ld_dir=./ld/chr${chr_i} chunk_size=500 gre_path=./h2-GRE/gre/gre.py python ${gre_path} mean-std --bfile=${bfile} --chunk-size=${chunk_size} --ld-dir=${ld_dir} qsub mean_std.sh This step will produce a file containing the mean and standard deviation for each genotype file. In the following few steps, we compute the LD matrix efficiently. We achieve this by seperating the big task, e.g., computing the 60k x 60k SNP covariance matrix into computing 3600 1k x 1k SNP covariance matrix.","title":"Compute the mean and standard deviation for each genotype file"},{"location":"biobank_demo/#cutting-the-task-of-computing-ld-matrix-into-several-tasks","text":"for chr_i in $(seq 1 22) do python ${gre_path} cut-ld --bfile ./genotype/chr${chr_i} --chunk-size 1000 --ld-dir ./ld/chr${chr_i} python ${gre_path} check-ld ./ld/chr${chr_i} done This will output two files part.info , part.missing in each directory ./ld/chr${chr_i} , which specifies the sub tasks needed to compute the LD.","title":"Cutting the task of computing LD matrix into several tasks"},{"location":"biobank_demo/#computing-each-part-of-covariance-matrix","text":"First create two scripts, calc_ld.sh , run_calc_ld.sh . calc_ld.sh #!/bin/sh #$ -cwd #$ -l h_data=16G,h_rt=0:40:00 #$ -j y #$ -o ./job_out chr_i=$1 batch_size=$2 bfile=./genotype/chr${chr_i} ld_dir=./ld/${chr_i} gre_path=./h2-GRE/gre/gre.py for i in $(seq 1 ${batch_size}) do line=$((SGE_TASK_ID + i - 1)) # part.missing contains undone tasks task_id=$(awk 'NR == n' n=$line ${ld_dir}/part.missing) python ${gre_path} calc-ld --bfile=${bfile} --part-i=${task_id} --ld-dir=${ld_dir} done run_calc_ld.sh batch_size=25 for chr_i in $(seq 1 22) do ld_dir=./ld/${chr_i} part_num=$(wc -l < ${ld_dir}/part.missing) if [ ${part_num} -ne 0 ]; then qsub -t 1-${part_num}:${batch_size} calc_ld.sh ${chr_i} ${batch_size} fi done done Then run the run_calc_ld.sh to submit the jobs. bash run_calc_ld.sh","title":"Computing each part of covariance matrix"},{"location":"biobank_demo/#check-if-jobs-are-done-and-resubmit-the-jobs-if-necessary","text":"Some of jobs might not finish due to slow nodes on the cluster. We update the part.missing list first. for chr_i in $(seq 1 22) do python ${gre_path} check-ld --ld-dir ./ld/chr${chr_i} done And calculate the ld for each part by bash run_calc_ld.sh . Repeat this until you see no tasks being submitted.","title":"check if jobs are done and resubmit the jobs if necessary"},{"location":"biobank_demo/#merging-parts-of-the-covariance-matrix","text":"Now we are close to finishing it! We will merge each parts of LD and compute the pseudo-inverse of LD. merge_ld.sh #!/bin/sh #$ -cwd #$ -j y #$ -o ./job_out #$ -m a bfile=$1 ld_dir=$2 gre_path=./h2-GRE/gre/gre.py python ${gre_path} merge-ld --bfile=${bfile} --ld-dir=${ld_dir} for chr_i in $(seq 1 22) do ld_dir=./ld/chr${chr_i} bfile=./genotype/chr${chr_i} if [ $chr_i -le 6 ] then qsub -l h_data=60G,h_rt=4:00:00,highp merge_ld.sh ${bfile} ${ld_dir} elif [ $chr_i -ge 7 -a $chr_i -le 12 ] then qsub -l h_data=40G,h_rt=3:00:00,highp merge_ld.sh ${bfile} ${ld_dir} elif [ $chr_i -ge 13 -a $chr_i -le 20 ] then qsub -l h_data=24G,h_rt=2:00:00,highp merge_ld.sh ${bfile} ${ld_dir} else qsub -l h_data=10G,h_rt=1:00:00,highp merge_ld.sh ${bfile} ${ld_dir} fi done Now we have all the ingredients needed for GRE and we are ready to get the SNP-heritability.","title":"Merging parts of the covariance matrix"},{"location":"biobank_demo/#estimating-snp-heritability","text":"Converting the sumstats in plink format into LDSC format. for chr_i in $(seq 1 22) do python ./h2-GRE/utils/utils.py sumstats-plink2ldsc --plink-sumstats ./assoc/chr${chr_i}.assoc.linear --legend ./genotype/chr${chr_i}.bim --out ./assoc/chr${chr_i}.ldsc.txt done Estimate SNP-heritability estimate.sh #!/bin/sh #$ -cwd #$ -l h_data=24G,h_rt=2:00:00,highp #$ -j y #$ -o ./job_out #$ -t 1-22:1 chr_i=${SGE_TASK_ID} legend=./genotype/chr${chr_i}.bim ld_dir=./ld/chr${chr_i} sumstats=./assoc/chr${chr_i}.ldsc.txt python gre.py estimate --legend=${legend} --ld-dir=${ld_dir} --sumstats=${sumstats} > ./estimate/chr${chr_i}.txt qsub estimate.sh ./estimate/chr${chr_i}.txt contains the estimate and standard deviation of GRE for each chromosome. Adding these up provides you the genome-wide SNP-heritability!","title":"Estimating SNP-heritability"},{"location":"estimate_h2gre/","text":"Estimating heritability using GRE \u00b6 After we get the inverse of the LD, we are ready to apply GRE to the OLS sumstats data. The GRE method requires that the sumstats and LD computed using the same genotype reference panel. The sumstats should be in LDSC sumstats format . Then we can apply the GRE estimator to the sumstats. legend=/path/to/bim_file ld_dir=./ld sumstats=/path/to/sumstats python gre.py estimate --legend=${legend} --ld-dir=${ld_dir} --sumstats=${sumstats} In the above we demonstrate applying GRE to the sumstats on one chromosome. To get the genome-wide h^2_{\\text{GRE}} estimate, we can add up the estimates and the variance in each chromosome.","title":"Calculating the h2gre"},{"location":"estimate_h2gre/#estimating-heritability-using-gre","text":"After we get the inverse of the LD, we are ready to apply GRE to the OLS sumstats data. The GRE method requires that the sumstats and LD computed using the same genotype reference panel. The sumstats should be in LDSC sumstats format . Then we can apply the GRE estimator to the sumstats. legend=/path/to/bim_file ld_dir=./ld sumstats=/path/to/sumstats python gre.py estimate --legend=${legend} --ld-dir=${ld_dir} --sumstats=${sumstats} In the above we demonstrate applying GRE to the sumstats on one chromosome. To get the genome-wide h^2_{\\text{GRE}} estimate, we can add up the estimates and the variance in each chromosome.","title":"Estimating heritability using GRE"},{"location":"faq/","text":"FAQ \u00b6 TODO","title":"FAQ"},{"location":"faq/#faq","text":"TODO","title":"FAQ"},{"location":"input_data/","text":"Input data \u00b6 Requirement on #SNPs and #individuals \u00b6 In order for the desirable properties of the h2-GRE estimator to hold, h2-GRE requires individual-level genotype and phenotype data where the number of individuals (N) is larger than the number of genotyped SNPs per chromosome (m_k for the k-th chromosome). As a reference point, in our analyses of the UK Biobank genotyped SNPs (UK Biobank Axiom Array), chromosome 1 (the largest chromosomes) had about 45-55K SNPs depending on how we QC-ed the data, while number of individuals is about 300K. Prepare the data \u00b6 The genotype and phenotypes should be in PLINK format. We will also use PLINK software to perform OLS regression. Note Here we assume that you know the basic about plink, e.g. basic commands, data format. You can read more on the plink website about covariate file , phenotype file and association analysis . We will use the following data. all_bfile : Genotype for the unrelated individuals in UK Biobank after quality control. This contains genome-wide data in one file. pheno_file : the phenotype file. covar_file : the covariates which will be used for association studies.","title":"Input Data"},{"location":"input_data/#input-data","text":"","title":"Input data"},{"location":"input_data/#requirement-on-snps-and-individuals","text":"In order for the desirable properties of the h2-GRE estimator to hold, h2-GRE requires individual-level genotype and phenotype data where the number of individuals (N) is larger than the number of genotyped SNPs per chromosome (m_k for the k-th chromosome). As a reference point, in our analyses of the UK Biobank genotyped SNPs (UK Biobank Axiom Array), chromosome 1 (the largest chromosomes) had about 45-55K SNPs depending on how we QC-ed the data, while number of individuals is about 300K.","title":"Requirement on #SNPs and #individuals"},{"location":"input_data/#prepare-the-data","text":"The genotype and phenotypes should be in PLINK format. We will also use PLINK software to perform OLS regression. Note Here we assume that you know the basic about plink, e.g. basic commands, data format. You can read more on the plink website about covariate file , phenotype file and association analysis . We will use the following data. all_bfile : Genotype for the unrelated individuals in UK Biobank after quality control. This contains genome-wide data in one file. pheno_file : the phenotype file. covar_file : the covariates which will be used for association studies.","title":"Prepare the data"},{"location":"ld/","text":"Computing chromosome wide inverse LD \u00b6 h2-GRE requires the inverse LD matrix for each chromosome. For a small set of SNPs, computing the inverse LD matrix is straightforward; for example, given a genotype matrix in PLINK format, from pysnptools.snpreader import Bed import numpy as np from scipy import linalg genotype = Bed('/path/to/bfile', count_A1=False).read().val ld = np.corrcoef(genotype) inv_ld = linalg.pinv(ld) However, there are typically up to ~60K genotyped SNPs per chromosome and >100K individuals. In order to compute LD in memory, we divide the above into the several steps and parallelize over chunks of SNPs. The following code computes the LD and inverse LD of a genotype matrix (in PLINK format). We assume that the genotype file only contains the information for one chromosome and the results will be at ld_dir . We will use the following variables below: ld_dir : location where our results store bfile : plink genotype file for only one chromosome mean_chunk_size : when computing mean and standard deviation, how many SNPs we compute at a time, change this parameter according to memory. ld_chunk_size : when computing the LD, how many SNPs we compute at a time, change this parameter according to memory. 1. Computing the mean and standard deviation of each SNP genotype \u00b6 The mean and standard deviation of each genotype column is needed to standardize the genotype matrix. # when number of individuals = ~350K, setting mean_chunk_size=500 takes up about 8G memory python gre.py mean-std --bfile=${bfile} --chunk-size=${mean_chunk_size} --ld-dir=${ld_dir} 2. Dividing the LD computations into multiple tasks \u00b6 Next, we compute the LD matrix which has dimensions number-of-SNPs x number-of-SNPs. For example, we can cut the large task of computing a 60k x 60k SNP covariance matrix into 3600 small tasks, each of which computes a 1k x 1k SNP covariance matrix. # this step will create a file `part.info` in ${ld_dir}, which contains the information for each sub-task python gre.py cut-ld --bfile=${bfile} --chunk-size=${ld_chunk_size} --ld-dir=${ld_dir} python gre.py check-ld ${ld_dir} Now we compute covariance matrix for each part. You can compute the covariance matrix sequentially as follows: part_num=$(wc -l < ${ld_dir}/part.info) for part_i in $(seq 1 ${part_num}) do python gre.py calc-ld --bfile=${bfile} --part-i=${part_i} --ld-dir=${ld_dir} done 3. Computing chromosome-wide inverse LD \u00b6 We merge the covariance matrices from the previous step to obtain the chromosome-wide LD matrices. First, check if each task is complete. If a task is not complete, check ${ld_dir}/part.missing and re-submit these. For a 60k x 60k covariance matrix, computing the inverse via singular value decomposition (SVD) takes about 60G memory and 4 hours. For a 10k x 10k covariance matrix, this step requires 6G memory and 1 hour. Note that the memory and time complexity are independent of the number of individuals. part_num=$(wc -l < ${ld_dir}/part.info) if [ -e ${ld_dir}/part.missing ]; then rm ${ld_dir}/part.missing fi for part_i in $(seq 1 ${part_num}) do if [ ! -e ${ld_dir}/part_${part_i}.npy ]; then echo ${part_i} >> ${ld_dir}/part.missing fi done if [ -e ${ld_dir}/part.missing ] then echo \"the following parts are missing, please rerun calc_ld.sh\" cat ${ld_dir}/part.missing else python gre.py merge-ld --bfile=${bfile} --ld-dir=${ld_dir} fi After this step, you will see the inverse of LD matrix and the rank in folder ${ld_dir} .","title":"Computing the inverse LD matrix"},{"location":"ld/#computing-chromosome-wide-inverse-ld","text":"h2-GRE requires the inverse LD matrix for each chromosome. For a small set of SNPs, computing the inverse LD matrix is straightforward; for example, given a genotype matrix in PLINK format, from pysnptools.snpreader import Bed import numpy as np from scipy import linalg genotype = Bed('/path/to/bfile', count_A1=False).read().val ld = np.corrcoef(genotype) inv_ld = linalg.pinv(ld) However, there are typically up to ~60K genotyped SNPs per chromosome and >100K individuals. In order to compute LD in memory, we divide the above into the several steps and parallelize over chunks of SNPs. The following code computes the LD and inverse LD of a genotype matrix (in PLINK format). We assume that the genotype file only contains the information for one chromosome and the results will be at ld_dir . We will use the following variables below: ld_dir : location where our results store bfile : plink genotype file for only one chromosome mean_chunk_size : when computing mean and standard deviation, how many SNPs we compute at a time, change this parameter according to memory. ld_chunk_size : when computing the LD, how many SNPs we compute at a time, change this parameter according to memory.","title":"Computing chromosome wide inverse LD"},{"location":"ld/#1-computing-the-mean-and-standard-deviation-of-each-snp-genotype","text":"The mean and standard deviation of each genotype column is needed to standardize the genotype matrix. # when number of individuals = ~350K, setting mean_chunk_size=500 takes up about 8G memory python gre.py mean-std --bfile=${bfile} --chunk-size=${mean_chunk_size} --ld-dir=${ld_dir}","title":"1. Computing the mean and standard deviation of each SNP genotype"},{"location":"ld/#2-dividing-the-ld-computations-into-multiple-tasks","text":"Next, we compute the LD matrix which has dimensions number-of-SNPs x number-of-SNPs. For example, we can cut the large task of computing a 60k x 60k SNP covariance matrix into 3600 small tasks, each of which computes a 1k x 1k SNP covariance matrix. # this step will create a file `part.info` in ${ld_dir}, which contains the information for each sub-task python gre.py cut-ld --bfile=${bfile} --chunk-size=${ld_chunk_size} --ld-dir=${ld_dir} python gre.py check-ld ${ld_dir} Now we compute covariance matrix for each part. You can compute the covariance matrix sequentially as follows: part_num=$(wc -l < ${ld_dir}/part.info) for part_i in $(seq 1 ${part_num}) do python gre.py calc-ld --bfile=${bfile} --part-i=${part_i} --ld-dir=${ld_dir} done","title":"2. Dividing the LD computations into multiple tasks"},{"location":"ld/#3-computing-chromosome-wide-inverse-ld","text":"We merge the covariance matrices from the previous step to obtain the chromosome-wide LD matrices. First, check if each task is complete. If a task is not complete, check ${ld_dir}/part.missing and re-submit these. For a 60k x 60k covariance matrix, computing the inverse via singular value decomposition (SVD) takes about 60G memory and 4 hours. For a 10k x 10k covariance matrix, this step requires 6G memory and 1 hour. Note that the memory and time complexity are independent of the number of individuals. part_num=$(wc -l < ${ld_dir}/part.info) if [ -e ${ld_dir}/part.missing ]; then rm ${ld_dir}/part.missing fi for part_i in $(seq 1 ${part_num}) do if [ ! -e ${ld_dir}/part_${part_i}.npy ]; then echo ${part_i} >> ${ld_dir}/part.missing fi done if [ -e ${ld_dir}/part.missing ] then echo \"the following parts are missing, please rerun calc_ld.sh\" cat ${ld_dir}/part.missing else python gre.py merge-ld --bfile=${bfile} --ld-dir=${ld_dir} fi After this step, you will see the inverse of LD matrix and the rank in folder ${ld_dir} .","title":"3. Computing chromosome-wide inverse LD"},{"location":"ols/","text":"Perform Association Study, i.e. OLS regression \u00b6 After getting the data, we are now ready to perform the OLS regression. To use GRE, be sure to perform the association studies with the exactly same genotype you use to compute the LD. We leave GRE with reference panel LD for future work. num_cols=$(head -1 $covar_file | awk '{print NF}') num_cols=$((num_cols-2)) for chr_i in $(seq 1 22) do plink \\ --allow-no-sex \\ --bfile ${all_bfile} \\ --chr ${chr_i} \\ --ci 0.95 \\ --covar ${covar_file} \\ --covar-number 1-${num_cols} \\ --linear hide-covar \\ --out ./assoc/chr${chr_i} \\ --pheno ${pheno_file} done You will get 22 association files after this step. Each contains the OLS sumstats for SNPs for 22 chromosomes.","title":"OLS regression"},{"location":"ols/#perform-association-study-ie-ols-regression","text":"After getting the data, we are now ready to perform the OLS regression. To use GRE, be sure to perform the association studies with the exactly same genotype you use to compute the LD. We leave GRE with reference panel LD for future work. num_cols=$(head -1 $covar_file | awk '{print NF}') num_cols=$((num_cols-2)) for chr_i in $(seq 1 22) do plink \\ --allow-no-sex \\ --bfile ${all_bfile} \\ --chr ${chr_i} \\ --ci 0.95 \\ --covar ${covar_file} \\ --covar-number 1-${num_cols} \\ --linear hide-covar \\ --out ./assoc/chr${chr_i} \\ --pheno ${pheno_file} done You will get 22 association files after this step. Each contains the OLS sumstats for SNPs for 22 chromosomes.","title":"Perform Association Study, i.e. OLS regression"},{"location":"simulation/","text":"We also provide code for simulating OLS sumstats with different genetic architecture. Simulating OLS sumstats can be divided into the following steps: computing the mean and standard deviation for each SNP in population and standardize the genotype matrix draw the effect sizes for each SNP simulating the phenotypes simulating the OLS sumstats geno = Bed('/path/to/bfile', count_A1=False).read().val # impute nan SNPs nanidx = np.where(np.isnan(genotype)) mean = np.nanmean(genotype, axis=0) genotype[nanidx] = mean_geno[nanidx[1]] # standardize genotypes geno -= geno.mean(axis=0) geno /= geno.std(axis=0) num_indv, num_snps = geno.shape # here we draw from infinitesimal effect sizes model with heritability 0.1 for an example beta = np.random.normal(loc=0.0, scale=1.0, size=num_snps) # simulating phenotypes phe_g = np.dot(geno, beta) phe_g = phe_g * np.sqrt(hsq / np.var(phe_g)) phe_e = np.random.normal(loc=0.0, scale=1.0, size=num_indv) phe_e = phe_e * np.sqrt((1 - hsq) / np.var(phe_e)) phe = phe_g + phe_e # simulating OLS sumstats z-scores zsc = np.dot(geno.T, phe) / np.sqrt(num_indv) This code should work fine for small dataset. But in some cases, we may want to 1. Simulate using large dataset. 2. Specify how the effect sizes are drawn (different genetic architecture) We can use the provided code located at sim_gwas/chr in this repository to achieve these two easily. By Changing option: chunk_size in config.json , we can fit the computation into memory. By specifies params , we can change how the effect sizes are drawn. In order to simulate using the provided code, we can input the command as follows: 1. Mean and standard deviation \u00b6 config=/path/to/config python ./sim_gwas.py mean-std --config=${config} 2. Effect sizes \u00b6 config=/path/to/config python ./sim_gwas.py beta --config=${config} --param-i=${param_i} 3. phenotype from genetic component \u00b6 config=/path/to/config python ./sim_gwas.py phe-g --config=${config} --param-i=${param_i} 4. phenotype (with environmental component added) \u00b6 config=/path/to/config python ./sim_gwas.py phe --config=${config} --param-i=${param_i} 5. OLS sumstats \u00b6 config=/path/to/config python ./sim_gwas.py zsc --config=${config} --param-i=${param_i}","title":"Simulation"},{"location":"simulation/#1-mean-and-standard-deviation","text":"config=/path/to/config python ./sim_gwas.py mean-std --config=${config}","title":"1. Mean and standard deviation"},{"location":"simulation/#2-effect-sizes","text":"config=/path/to/config python ./sim_gwas.py beta --config=${config} --param-i=${param_i}","title":"2. Effect sizes"},{"location":"simulation/#3-phenotype-from-genetic-component","text":"config=/path/to/config python ./sim_gwas.py phe-g --config=${config} --param-i=${param_i}","title":"3. phenotype from genetic component"},{"location":"simulation/#4-phenotype-with-environmental-component-added","text":"config=/path/to/config python ./sim_gwas.py phe --config=${config} --param-i=${param_i}","title":"4. phenotype (with environmental component added)"},{"location":"simulation/#5-ols-sumstats","text":"config=/path/to/config python ./sim_gwas.py zsc --config=${config} --param-i=${param_i}","title":"5. OLS sumstats"}]}